{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning a language is difficult because language has many complex rules. If we want computers to be able to understand language, `we either need to explicitly teach computers the rules, or enable the computers to intuit the rules themselves.` The former is a lot like `learning a second language`, and the latter is a lot like `learning your native language`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Broadly speakingly, natural language processing is the study of enabling computers to understand human languages. This field may involve teaching computers **to automatically score essays, infer grammatical rules, or determine the emotions associated with text**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we feed a computer written text, it has no idea what that text means. In order for a computer to begin making inferences from it, we'll need to `convert the text to a numerical representation`. This process will enable the computer to intuit grammatical rules, which is more akin to learning a first language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our data set consists of submissions users made to Hacker News from 2006 to 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Developer Arnaud Drizard used the Hacker News API to scrape the data, which you can find in one of his GitHub repositories. We've sampled 3000 rows from the data randomly, and removed all of the extraneous columns. Our data only has four columns:\n",
    "  * submission_time - When the article was submitted\n",
    "  * upvotes - The number of upvotes the article received\n",
    "  * url - The base URL of the article\n",
    "  * headline - The article's headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission, we'll be predicting the number of upvotes the articles received, based on their headlines. Because upvotes are an indicator of popularity, we'll discover which types of articles tend to be the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "submission=pd.read_csv('sel_hn_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2014-06-24T05:50:40.000Z</th>\n",
       "      <th>1</th>\n",
       "      <th>flux7.com</th>\n",
       "      <th>8 Ways to Use Docker in the Real World</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-17T16:57:59Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blog.jonasbandi.net</td>\n",
       "      <td>Software: Sadly we did adopt from the construc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-02-04T02:36:30Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blogs.wsj.com</td>\n",
       "      <td>Google’s Stock Split Means More Control for L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-26T07:11:29Z</td>\n",
       "      <td>1</td>\n",
       "      <td>threatpost.com</td>\n",
       "      <td>SSL DOS attack tool released exploiting negoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-03T15:43:44Z</td>\n",
       "      <td>67</td>\n",
       "      <td>algorithm.com.au</td>\n",
       "      <td>Immutability and Blocks Lambdas and Closures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-13T16:49:20Z</td>\n",
       "      <td>1</td>\n",
       "      <td>winmacsofts.com</td>\n",
       "      <td>Comment optimiser la vitesse de Wordpress?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  2014-06-24T05:50:40.000Z   1            flux7.com  \\\n",
       "0     2010-02-17T16:57:59Z   1  blog.jonasbandi.net   \n",
       "1     2014-02-04T02:36:30Z   1        blogs.wsj.com   \n",
       "2     2011-10-26T07:11:29Z   1       threatpost.com   \n",
       "3     2011-04-03T15:43:44Z  67     algorithm.com.au   \n",
       "4     2013-01-13T16:49:20Z   1      winmacsofts.com   \n",
       "\n",
       "              8 Ways to Use Docker in the Real World  \n",
       "0  Software: Sadly we did adopt from the construc...  \n",
       "1   Google’s Stock Split Means More Control for L...  \n",
       "2  SSL DOS attack tool released exploiting negoti...  \n",
       "3       Immutability and Blocks Lambdas and Closures  \n",
       "4         Comment optimiser la vitesse de Wordpress?  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns=['submission_time','upvotes','url','headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-17T16:57:59Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blog.jonasbandi.net</td>\n",
       "      <td>Software: Sadly we did adopt from the construc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-02-04T02:36:30Z</td>\n",
       "      <td>1</td>\n",
       "      <td>blogs.wsj.com</td>\n",
       "      <td>Google’s Stock Split Means More Control for L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-26T07:11:29Z</td>\n",
       "      <td>1</td>\n",
       "      <td>threatpost.com</td>\n",
       "      <td>SSL DOS attack tool released exploiting negoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-04-03T15:43:44Z</td>\n",
       "      <td>67</td>\n",
       "      <td>algorithm.com.au</td>\n",
       "      <td>Immutability and Blocks Lambdas and Closures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-13T16:49:20Z</td>\n",
       "      <td>1</td>\n",
       "      <td>winmacsofts.com</td>\n",
       "      <td>Comment optimiser la vitesse de Wordpress?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        submission_time  upvotes                  url  \\\n",
       "0  2010-02-17T16:57:59Z        1  blog.jonasbandi.net   \n",
       "1  2014-02-04T02:36:30Z        1        blogs.wsj.com   \n",
       "2  2011-10-26T07:11:29Z        1       threatpost.com   \n",
       "3  2011-04-03T15:43:44Z       67     algorithm.com.au   \n",
       "4  2013-01-13T16:49:20Z        1      winmacsofts.com   \n",
       "\n",
       "                                            headline  \n",
       "0  Software: Sadly we did adopt from the construc...  \n",
       "1   Google’s Stock Split Means More Control for L...  \n",
       "2  SSL DOS attack tool released exploiting negoti...  \n",
       "3       Immutability and Blocks Lambdas and Closures  \n",
       "4         Comment optimiser la vitesse de Wordpress?  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2999, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=submission.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenizing the Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our goal is to train a linear regression algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation.\n",
    "* we'll use a bag of words model. [A bag of words model](https://en.wikipedia.org/wiki/Bag-of-words_model) represents each piece of text as a numerical vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`The first step in creating a bag of words model is tokenization. In tokenization, we break a sentence up into disconnected words.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting each sentence into a list of individual words, or tokens. The split occurs on the space character (\" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Split each headline into individual words on the space character(\" \"), and append the resulting list to tokenized_headlines.\n",
    "\n",
    "* When you're finished, tokenized_headlines should be a list of lists. Each list should contain the tokens for the headline located at the corresponding position in the submissions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Software:, Sadly, we, did, adopt, from, the, ...\n",
       "1       [, Google’s, Stock, Split, Means, More, Contro...\n",
       "2       [SSL, DOS, attack, tool, released, exploiting,...\n",
       "3       [Immutability, and, Blocks, Lambdas, and, Clos...\n",
       "4       [Comment, optimiser, la, vitesse, de, Wordpress?]\n",
       "5       [ilk, is, not, as, good, for, you, as, you, th...\n",
       "6        [Worldometers, -, Real, time, world, statistics]\n",
       "7       [icrosoft, strikes, back:, introduces, docs, f...\n",
       "8                            [Net, HTTP, status, codes, ]\n",
       "9       [Anecdata, or, how, McKinsey’s, story, became,...\n",
       "11            [Immigration, Overhaul, Passes, in, Senate]\n",
       "12           [What, matters, most, at, Ad:TECH, SF, 2014]\n",
       "13      [Amazon, Silk, revisited:, Is, the, split, clo...\n",
       "14      [Dieter, Ram's, Ten, Principles, Of, Good, Des...\n",
       "15                                          [Gmail, Down]\n",
       "16      [Show, How, Don't, Tell, What, -, A, Managemen...\n",
       "17      [U.S., releases, images, said, to, implicate, ...\n",
       "18      [Real-time, OCR/translation, of, Chinese, text...\n",
       "19                [Des, dcorations, de, Nol, plus, colos]\n",
       "20                                  [Childish, behaviour]\n",
       "21      [Silicon, Valley, shouldnt, fear, China, Atlan...\n",
       "22                        [, , Mirillis, Action, 1.3.3.0]\n",
       "23      [Older, entrepreneurs, find, new, niche, in, s...\n",
       "24      [Computer, Programming, for, Children, Minus, ...\n",
       "25                 [AB, testing, with, Google, Analytics]\n",
       "26                                     [Latino, SexDates]\n",
       "27      [otorola, is, turning, Android, 4.0, into, a, ...\n",
       "28            [, A.I., movie, merch:, Super, Toy, Teddy ]\n",
       "29      [Prisoners, in, GA, Organize, Strike, via, Con...\n",
       "30           [Amazon, Cloud, Major, Outages, -, Analysis]\n",
       "                              ...                        \n",
       "2969                                [Twitter, Experiment]\n",
       "2970    [Case, Study:, How, Orabrush, Got, into, Walmart]\n",
       "2971    [Don't, Miss, Out, on, the, Quora, Distributio...\n",
       "2972    [Designing, Icons, for, the, Apple, Watch:, Ic...\n",
       "2973    [Show, HN:, VeriFi, -, A, web, service, for, a...\n",
       "2974    [FM, Stream:, broadcasting, local, radio, to, ...\n",
       "2975    [Gist, (interesting, social, media, realm, sta...\n",
       "2976                         [Nike, DunksNike, Free, Run]\n",
       "2977        [Why, Are, Americans, So, Bad, At, Saving?, ]\n",
       "2978     [DevOps:, Improved, Productivity, Higher, Value]\n",
       "2979    [Can, I, live, in, NYC, for, a, month, on, a, ...\n",
       "2980    [Technology, Cannot, Disrupt, Education, From,...\n",
       "2981                        [Facebook, Acquires, Drop.io]\n",
       "2982         [Practical, and, creative, uses, of, jQuery]\n",
       "2983    [Tim, Cook, and, Apple, celebrate, #ApplePride...\n",
       "2984    [Thomas, Sabo, Motorcycle, Pendant, T0188-001-...\n",
       "2985    [Breaking, a, MacBook, in, memory, of, Steve, ...\n",
       "2986        [Self-Enforcing, Contracts, And, Factum, Law]\n",
       "2987      [Programmers, sought, for, tropical, hackathon]\n",
       "2988    [Why, web, frameworks, are, a, bad, thing, par...\n",
       "2989            [The, problem, with, CSS, pre-processors]\n",
       "2990                            [Animated, Bzier, Curves]\n",
       "2991                                [Business, Headshots]\n",
       "2992    [Girls, like, modern, technology., Like, telep...\n",
       "2993                   [Walks, And, Walking, Accessories]\n",
       "2994        [Rot.js:, ROguelike, Toolkit, in, JavaScript]\n",
       "2995    [Amazon, auctions, computing, power:, Clouds, ...\n",
       "2996    [Nissan, CEO:, We, will, have, an, autonomous,...\n",
       "2997              [Connecting, to, Dropbox, with, Python]\n",
       "2998    [Crowdsourcing, disaster, response, What, we, ...\n",
       "Name: headline, Length: 2800, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_headlines=[]\n",
    "submission['headline'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headlines = []\n",
    "for item in submission[\"headline\"]:\n",
    "    tokenized_headlines.append(item.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Software:',\n",
       " 'Sadly',\n",
       " 'we',\n",
       " 'did',\n",
       " 'adopt',\n",
       " 'from',\n",
       " 'the',\n",
       " 'construction',\n",
       " 'analogy']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_headlines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing Tokens to Increase Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now have tokens, but we need to process them a bit to make our predictions more accurate. We know that Berlin, Berlin., and berlin all refer to the same word, but the computer doesn't know that. We'll need to convert those variations so that they're consistent.\n",
    "\n",
    "* We can do this by lowercasing (which will convert Berlin to berlin), and also by removing punctuation (so Berlin. becomes Berlin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "* Loop through each item in tokenized_headlines, which is a list of lists.\n",
    "  * For each list of tokens:\n",
    "    * Convert each individual token to lowercase\n",
    "    * Remove all of the items in the punctuation list from each individual token\n",
    "    * Append the clean list to clean_tokenized\n",
    "* clean_tokenized should now be a list of lists. Each list should contain the preprocessed tokens associated with the headline in the corresponding position of the submissions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tokenized_headlines:\n",
    "    tokens=[]\n",
    "    for token in item:\n",
    "        token=token.lower()\n",
    "        for punc in punctuation:\n",
    "            token=token.replace(punc,'')\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['software',\n",
       "  'sadly',\n",
       "  'we',\n",
       "  'did',\n",
       "  'adopt',\n",
       "  'from',\n",
       "  'the',\n",
       "  'construction',\n",
       "  'analogy'],\n",
       " ['googles',\n",
       "  'stock',\n",
       "  'split',\n",
       "  'means',\n",
       "  'more',\n",
       "  'control',\n",
       "  'for',\n",
       "  'larry',\n",
       "  'and',\n",
       "  'sergey'],\n",
       " ['ssl',\n",
       "  'dos',\n",
       "  'attack',\n",
       "  'tool',\n",
       "  'released',\n",
       "  'exploiting',\n",
       "  'negotiation',\n",
       "  'overhead']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokenized[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Assembling a Matrix of Unique Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokens, we can begin converting the sentences to their numerical representations.` First, we'll retrieve all of the unique words from all of the headlines.` Then, we'll create a matrix, and `assign those words as the column headers`. We'll `initialize all of the values in the matrix to 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a pandas dataframe instead of a NumPy matrix. We can create a dataframe with all zero values using this syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Find all of the unique tokens in clean_tokenized, and assign the result to unique_tokens.\n",
    "  * Only add tokens that occur more than once (across all of the headlines). Tokens that only occur once don't add anything to the model's prediction power, and removing them will make our algorithm run much more quickly. \n",
    "  * To do this, you can keep a list of the tokens that occur once in the data, and a different list of the tokens that occur more than once. If a token is already in the first list when you encounter it and it's not in the second list, you should add it to the second list.\n",
    "  * When you're finished, unique_tokens should contain any tokens that occur more than once across all of the headlines.\n",
    "  * Each token in unique_tokens should only appear in the list a single time.\n",
    " \n",
    "* Create a dataframe with as many rows as there are items in the clean_tokenized list. Each column name should be a token in unique_tokens. Initialize all of the cells to the value 0. Assign the dataframe to the variable counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens=[]\n",
    "single_tokens=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in clean_tokenized:\n",
    "    for token in item:\n",
    "        if token in \\:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
