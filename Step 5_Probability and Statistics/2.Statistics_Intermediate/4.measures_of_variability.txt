
1. The range
   > we've focused entirely on summarizing distributions using the mean, the weighted mean, the median, and the mode.
   > A=[4,4,4,4],B=[0,8,0,8] A have equal mean,mode and meadian and same for B with no mode.
   > One intuitive way to measure the variability of a distribution is to find the difference between the maximum and the minimum value.
     We call this measure of variability the range.
2. The Average Distance
   > The problem with the range is that it considers only two values in the distribution — the minimum and the maximum value
   > The root of the problem is that the range considers only the two extreme values, and this makes it extremely sensitive to outliers
      [1,1,1,1,1,1,1,21]
   > To take into account each value when measuring variability we could:
     Take a reference value, and measure the distance of each value in the distribution from that reference value.
       We can take the mean of the distribution as a reference value.
       Then, we measure the distance between each value in the distribution and the mean.
     Find the mean of the distances.
       We first need to sum up all the distances.
       Then we need to divide the total by the number of distances.

3. Mean Absolute Deviation
   >  the total distance of the values that are above the mean is the same as the total distance of the values below the mean.It makes sum 0.
   > To solve this problem, we can take the absolute value of each distance, and then sum up the absolute values.
   > We call this measure of variability mean absolute distance. In statistical jargon, however, the distance of a value from the mean is called deviation.
     So the mean absolute distance is more commonly known as mean absolute deviation or average absolute deviation.

4. Variance
  > In the previous screen we transformed the distances to absolute values to avoid having the sum of distances amount to 0 in the numerator. 
    Another way to solve this problem is to square each distance and then find the mean of all the squared distances:
  > This measure of variability is sometimes called mean squared distance or mean squared deviation (remember that "distance" and 
    "deviation" are synonymous in this context). However, it's more commonly known as variance.
  > Squaring the distances or taking their absolute values ensure that we get a variability value that is greater than 0 for 
    all distributions that show some variability. 
    Notice, however, that variance and mean absolute deviation will still be 0 for distributions that show no variability.
    D=[2,2,2] variance=0 , mean absolute deviation=0
  
5. standard Deviation
   > This high variability value is the direct result of the squaring process, which makes most distances much bigger than they actually are.
     D=[1,1,1,1,1,1,1,1,1,21] variance=36 which is very high
   > Squaring the distances also has the drawback of squaring the units of measuremen
   > To solve this problem and also reduce the variability value, we can take the square root of variance.
   > The square root of variance is called standard deviation 

6. Average variability Around the Mean
   > Standard deviation tells us how much the values in a distribution vary (on average) around the mean of that distribution.
   > The standard deviation doesn't set boundaries for the values in a distribution
 

7. Measure of spread
  > Another way to understand standard deviation is as a measure of spread in a distribution — 
    values in a distribution can be more or less spread
  
8. The Sample Standard Deviation
   > In practice, mu is almost never known, and we can't find it from our sample either, but we can estimate mu using the sample mean x bar.
   >  One way we can check this is by sampling repeatedly a known population and see how the sample standard deviations 
      compare on average to the population standard deviation.
   
9. Bessels's Correction
   > Some sample standard deviations are lower than the population standard deviation, some are greater, 
     some may even be equal to the population standard deviation,but on average the sample standard deviation is 
     lower than the population standard deviation.
   > We can get a good intuition for why the sample standard deviation underestimates if we think in terms of distribution spread. 
    When we sample a population, it's generally more likely to get a sample with a spread that's lower than the population's spread. 
    This generally translates to a lower standard deviation than in the population.
   > To correct the underestimation problem, we can try to slightly modify the sample standard deviation formula to return higher values
   > small correction we added to the sample standard deviation (dividing by n-1 instead of n) is called Bessel's correction.

10. Standard Notation
    > We could decrease the denominator more (dividing by  maybe) to try improving the correction. 
     However, we need a single mathematical definition for the sample standard deviation, and we have to choose between n,n-1,n-2, etc.
     Remember that in practice we don't know the population standard deviation, so we can't tell which correction would work best 
     for each sample standard deviation.
    > Statisticians agree that n-1 is the best choice for the sample standard deviation formula
    > The main takeaway is that we need to use the  and  formulae (with Bessel's correction) for samples. 
      For populations, we can use the  or  formulae (without Bessel's correction).

11. Sample Variance — Unbiased Estimator
   > argument supporting using n-1 comes from the fact that the sample variance  (which uses ) is an unbiased estimator for the population
     variance . Since standard deviation is just the square root of variance, it makes sense to use  as well 
     (although standard deviation is not an unbiased estimator)
   > we call a statistic an unbiased estimator when that statistic is equal on average to the parameter it estimates.
   > The sample variance  is an unbiased estimator for the population variance  only when we sample with replacement


In this mission, we learned how to measure the variability of a distribution using the range, the mean absolute deviation, 
the variance, and the standard deviation. These metrics are ideal for measuring the variability of distributions whose values
are measured on an interval or ratio scale.