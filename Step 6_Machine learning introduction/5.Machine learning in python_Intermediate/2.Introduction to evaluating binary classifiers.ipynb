{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous mission, we learned about classification, logistic regression, and how to use scikit-learn to fit a logistic regression model to a dataset on graduate school admissions. We'll continue to work with the dataset, which contains data on 644 applications with the following columns:\n",
    "\n",
    "* gre - applicant's score on the Graduate Record Exam, a generalized test for prospective graduate students.\n",
    "Score ranges from 200 to 800.\n",
    "* gpa - college grade point average.\n",
    "Continuous between 0.0 and 4.0.\n",
    "* admit - binary value\n",
    "Binary value, 0 or 1, where 1 means the applicant was admitted to the program and 0 means the applicant was rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Use the LogisticRegression method predict to return the label for each observation in the dataset, admissions. Assign the returned list to labels.\n",
    "* Add a new column to the admissions Dataframe named predicted_label that contains the values from labels.\n",
    "* Use the Series method value_counts and the print function to display the distribution of the values in the predicted_label column.\n",
    "* Use the Dataframe method head and the print function to display the first 5 rows in admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "admissions=pd.read_csv('admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression(solver='lbfgs')\n",
    "lr.fit(admissions[['gpa']],admissions['admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=lr.predict(admissions[['gpa']])\n",
    "admissions['predicted_label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    507\n",
       "1    137\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit       gpa         gre  predicted_label\n",
       "0      0  3.177277  594.102992                0\n",
       "1      0  3.412655  631.528607                0\n",
       "2      0  2.728097  553.714399                0\n",
       "3      0  3.093559  551.089985                0\n",
       "4      0  3.141923  537.184894                0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to determine the effectiveness of a classification model is **prediction accuracy**. Accuracy helps us answer the question:\n",
    "\n",
    "**What fraction of the predictions were correct (actual label matched predicted label)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Prediction accuracy boils down to the number of labels that were correctly predicted divided by the total number of observations:`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Accuracy = \\dfrac{\\text{# of Correctly Predicted}}{\\text{# of Observations}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To decide who gets admitted, we set a threshold and accept all of the students where their computed probability exceeds that threshold. **`This threshold is called the discrimination threshold`** and scikit-learn sets it to 0.5 by default when predicting labels. If the predicted probability is greater than 0.5, the label for that observation is 1. If it is instead less than 0.5, the label for that observation is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Rename the admit column from the admissions Dataframe to actual_label so it's more clear which column contains the predicted labels (predicted_label) and which column contains the actual labels (actual_label).\n",
    "* Compare the predicted_label column with the actual_label column.\n",
    "  * Use a double equals sign (==) to compare the 2 Series objects and assign the resulting Series object to matches.\n",
    "* Use conditional filtering to filter admissions to just the rows where matches is True. Assign the resulting Dataframe to correct_predictions.\n",
    "  * Display the first 5 rows in correct_predictions to make sure the values in the predicted_label and actual_label columns are equal.\n",
    "* Calculate the accuracy and assign the resulting float value to accuracy.\n",
    "  * Display accuracy using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions.rename(columns={'admit':'actual_label'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label       gpa         gre  predicted_label\n",
       "0             0  3.177277  594.102992                0\n",
       "1             0  3.412655  631.528607                0\n",
       "2             0  2.728097  553.714399                0\n",
       "3             0  3.093559  551.089985                0\n",
       "4             0  3.141923  537.184894                0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches=admissions['actual_label']==admissions['predicted_label']\n",
    "correct_predictions=admissions[matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label       gpa         gre  predicted_label\n",
       "0             0  3.177277  594.102992                0\n",
       "1             0  3.412655  631.528607                0\n",
       "2             0  2.728097  553.714399                0\n",
       "3             0  3.093559  551.089985                0\n",
       "4             0  3.141923  537.184894                0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6847826086956522\n"
     ]
    }
   ],
   "source": [
    "accuracy=len(correct_predictions)/len(admissions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Binary classification outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculating the accuracy of a model on the dataset used for training is a useful initial step just to make sure the model at least beats randomly assigning a label for each observation. However, prediction accuracy doesn't tell us much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`The accuracy doesn't tell us how the model performs on data it wasn't trained on`**. A model that returns a 100% accuracy when evaluated on it's training set doesn't tell us how well the model works on data it's never seen before (and wasn't trained on). `Accuracy also doesn't help us discriminate between the different types of outcomes a binary classification model can make.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission, we'll focus on the principles of evaluating binary classification models by testing our model's effectiveness on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<block><pre>\n",
    "To start, let's discuss the 4 different outcomes of a binary classification model:\n",
    "\n",
    "\t\n",
    " Prediction                   Observation\n",
    "                   Admitted (1)\t       Rejected (0)\n",
    "Admitted (1)\tTrue Positive (TP)\tFalse Positive (FP)\n",
    "Rejected (0)\tFalse Negative (FN)\tTrue Negative (TN)\n",
    "<block></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By segmenting a model's predictions into these different outcome categories, we can start to think about other measures of effectiveness that give us more granularity than simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Binary classification outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Extract all of the rows where predicted_label and actual_label both equal 1. Then, calculate the number of true positives and assign to true_positives.\n",
    "\n",
    "* Extract all of the rows where predicted_label and actual_label both equal 0. Then, calculate the number of true negatives and assign to true_negatives.\n",
    "\n",
    "* Display both true_positives and true_negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives_filter=(admissions['predicted_label']==1) & (admissions['actual_label']==1)\n",
    "true_positivess=admissions[true_positives_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>3.652733</td>\n",
       "      <td>662.854261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1</td>\n",
       "      <td>3.968276</td>\n",
       "      <td>621.354786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1</td>\n",
       "      <td>3.582525</td>\n",
       "      <td>574.138603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1</td>\n",
       "      <td>3.880944</td>\n",
       "      <td>628.912737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1</td>\n",
       "      <td>3.512116</td>\n",
       "      <td>653.744260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual_label       gpa         gre  predicted_label\n",
       "401             1  3.652733  662.854261                1\n",
       "409             1  3.968276  621.354786                1\n",
       "411             1  3.582525  574.138603                1\n",
       "412             1  3.880944  628.912737                1\n",
       "415             1  3.512116  653.744260                1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positivess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives=len(true_positivess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_negatives=len(admissions[(admissions['predicted_label']==0)&(admissions['actual_label'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "print(true_positives)\n",
    "print(true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sensitivity or True Positive Rate** - The proportion of applicants that were correctly admitted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $TPR=\\dfrac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all of the students that should have been admitted (True Positives + False Negatives), what fraction did the model correctly admit (True Positives)? More generally, this measure helps us answer the question:\n",
    "\n",
    "**How effective is this model at identifying positive outcomes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` If the True Positive Rate is low, it means that the model isn't effective at catching positive cases.` For certain problems, high sensitivity is incredibly important. `If we're building a model to predict which patients have cancer, every patient that is missed by the model could mean a loss of life.` We want a highly sensitive model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Calculate the number of false negatives (where the model predicted rejected but the student was actually admitted) and assign to false_negatives.\n",
    "* Calculate the sensitivity and assign the computed value to sensitivity.\n",
    "* Display sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives=len(admissions[(admissions['predicted_label']==0)&(admissions['actual_label']==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36475409836065575\n"
     ]
    }
   ],
   "source": [
    "sensitivity=true_positives/(true_positives+false_negatives)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of predicting student admissions, this probably isn't too bad of a thing.` Graduate schools can only admit a select number of students into their programs and by definition they end up rejecting many qualified students that would have succeeded.`\n",
    "\n",
    "In the healthcare context, `however, low sensitivity could mean a severe loss of life`. If a classification model is only catching 12.7% of positive cases for an illness, then around 7 of 8 people are going undiagnosed (being classified as false negatives). Hopefully you're beginning to acquire a sense for the tradeoffs predictive models make and the importance of understanding the various measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity or True Negative Rate** - The proportion of applicants that were correctly rejected:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $TNR=\\dfrac{\\text{True Negatives}}{\\text{False Positives} + \\text{True Negatives}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps in:\n",
    "\n",
    "**How effective is this model at identifying negative outcomes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A high specificity means that the model is really good at predicting which applicants should be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Calculate the number of false positives (where the model predicted admitted but the student was actually rejected) and assign to false_positives.\n",
    "* Calculate the specificity and assign the computed value to specificity.\n",
    "* Display specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives=len(admissions[(admissions['predicted_label']==1)&(admissions['actual_label'])==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21830985915492956\n"
     ]
    }
   ],
   "source": [
    "specificity=true_negatives/(true_negatives+false_positives)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mission, we learned about some of the different ways of evaluating how well a binary classification model performs. \n",
    "These measures are just a starting point, however, and aren't super useful by themselves. In the next mission, we'll dive into cross-validation, where we'll evaluate our model's accuracy on new data that it wasn't trained on. In addition, we'll explore how varying the discrimination threshold affects the measures we learned about in this mission. These important techniques help us gain a much more complete understanding of a classification model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
